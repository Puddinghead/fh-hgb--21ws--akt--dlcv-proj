{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import PIL\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_dir = pathlib.Path('./data')\n",
    "data = list(data_dir.glob('*/*.*'))\n",
    "img_height = 227\n",
    "img_width = 227\n",
    "\n",
    "# partitioning\n",
    "test_set_ratio = 0.1\n",
    "validation_set_ratio = 0.1\n",
    "\n",
    "# random\n",
    "random_seed = 42\n",
    "\n",
    "# keras\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = tf.keras.utils.image_dataset_from_directory(\n",
    "    batch_size = batch_size,\n",
    "    directory = data_dir,\n",
    "    image_size = (img_height, img_width), # resized image dimensions\n",
    "    seed = random_seed) # ensure known random_seed is used (for consistency)\n",
    "ds_all_class_names = ds_all.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_scale = 0.1\n",
    "img_col_count = 3\n",
    "img_row_count = 3\n",
    "\n",
    "plt.figure(figsize=(img_height * img_scale, img_width * img_scale))\n",
    "for images, labels in ds_all.take(1):\n",
    "    for i in range(img_col_count * img_row_count):\n",
    "        ax = plt.subplot(img_row_count, img_col_count, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(ds_all_class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_size = ds_all.__len__().numpy()\n",
    "print(\"ds_all_size        => \" + str(ds_all_size))\n",
    "\n",
    "# use first batches for test and validation as these are full batches (last batch might be partial batch)\n",
    "ds_test_size       = max(1, int(ds_all_size * test_set_ratio      ))\n",
    "ds_validation_size = max(1, int(ds_all_size * validation_set_ratio))\n",
    "print(\"ds_test_size       => \" + str(ds_test_size))\n",
    "print(\"ds_validation_size => \" + str(ds_validation_size))\n",
    "\n",
    "# training set becomes rest\n",
    "ds_training_size = ds_all_size - ds_validation_size - ds_test_size\n",
    "print(\"ds_training_size   => \" + str(ds_training_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test       = ds_all                                        .take(ds_test_size      )\n",
    "ds_validation = ds_all.skip(ds_test_size                     ).take(ds_validation_size)\n",
    "ds_training   = ds_all.skip(ds_test_size + ds_validation_size).take(ds_training_size  )\n",
    "\n",
    "ds_training_repeated = ds_training.repeat()\n",
    "\n",
    "# check sizes\n",
    "print(\"ds_test      .__len__() => \" + str(ds_test      .__len__().numpy()))\n",
    "print(\"ds_validation.__len__() => \" + str(ds_validation.__len__().numpy()))\n",
    "print(\"ds_training  .__len__() => \" + str(ds_training  .__len__().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get base model\n",
    "base_model = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top = False,\n",
    "    input_shape = (img_height, img_width, 3),\n",
    "    pooling = 'max', # ( None | avg | max )\n",
    "    weights = 'imagenet') # ( None | imagenet | path to weights file)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add convolution layers\n",
    "model_1 = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Dense(512, 'relu'),\n",
    "    tf.keras.layers.Dense(256, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'softmax')\n",
    "])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1.compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['sparse_categorical_accuracy'],\n",
    "    optimizer = 'adam')\n",
    "\n",
    "history = model_1.fit(\n",
    "    ds_training_repeated,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=5) # ( None (not with infinite datasets) | int )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
