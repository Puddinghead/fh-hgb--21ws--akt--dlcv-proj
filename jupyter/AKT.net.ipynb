{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import PIL\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_dir = pathlib.Path('./data')\n",
    "data = list(data_dir.glob('*/*.*'))\n",
    "img_height = 227\n",
    "img_width = 227\n",
    "\n",
    "# partitioning\n",
    "test_set_ratio = 0.1\n",
    "validation_set_ratio = 0.1\n",
    "\n",
    "# random\n",
    "random_seed = 42\n",
    "\n",
    "# keras\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = tf.keras.utils.image_dataset_from_directory(\n",
    "    batch_size = batch_size,\n",
    "    directory = data_dir,\n",
    "    image_size = (img_height, img_width), # resized image dimensions\n",
    "    seed = random_seed) # ensure known random_seed is used (for consistency)\n",
    "ds_all_class_names = ds_all.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_scale = 0.1\n",
    "img_col_count = 3\n",
    "img_row_count = 3\n",
    "\n",
    "def show_imgs(ds):\n",
    "    plt.figure(figsize=(img_height * img_scale, img_width * img_scale))\n",
    "    for images, labels in ds.take(1):\n",
    "        for i in range(img_col_count * img_row_count):\n",
    "            ax = plt.subplot(img_row_count, img_col_count, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(ds_all_class_names[labels[i]])\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs(ds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_size = ds_all.__len__().numpy()\n",
    "print(\"ds_all_size        => \" + str(ds_all_size))\n",
    "\n",
    "# use first batches for test and validation as these are full batches (last batch might be partial batch)\n",
    "ds_test_size       = max(1, int(ds_all_size * test_set_ratio      ))\n",
    "ds_validation_size = max(1, int(ds_all_size * validation_set_ratio))\n",
    "print(\"ds_test_size       => \" + str(ds_test_size))\n",
    "print(\"ds_validation_size => \" + str(ds_validation_size))\n",
    "\n",
    "# training set becomes rest\n",
    "ds_training_size = ds_all_size - ds_validation_size - ds_test_size\n",
    "print(\"ds_training_size   => \" + str(ds_training_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test       = ds_all                                        .take(ds_test_size      )\n",
    "ds_validation = ds_all.skip(ds_test_size                     ).take(ds_validation_size)\n",
    "ds_training   = ds_all.skip(ds_test_size + ds_validation_size).take(ds_training_size  )\n",
    "\n",
    "ds_training_repeated = ds_training.repeat()\n",
    "\n",
    "# check sizes\n",
    "print(\"ds_test      .__len__() => \" + str(ds_test      .__len__().numpy()))\n",
    "print(\"ds_validation.__len__() => \" + str(ds_validation.__len__().numpy()))\n",
    "print(\"ds_training  .__len__() => \" + str(ds_training  .__len__().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs(ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs(ds_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_1 = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top = False,\n",
    "    input_shape = (img_height, img_width, 3),\n",
    "    pooling = 'max', # ( None | avg | max )\n",
    "    weights = 'imagenet') # ( None | imagenet | path to weights file)\n",
    "\n",
    "for layer in base_model_1.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add convolution layers to base model\n",
    "model_1_1 = tf.keras.Sequential([\n",
    "    base_model_1,\n",
    "    tf.keras.layers.Dense(512, 'relu'),\n",
    "    tf.keras.layers.Dense(256, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'softmax')\n",
    "])\n",
    "\n",
    "model_1_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_1.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_1_1 = model_1_1.fit(\n",
    "    ds_training_repeated,\n",
    "    epochs = 10,\n",
    "    steps_per_epoch = 10) # ( None (not with infinite datasets) | int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1_1.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_1.save('akt_net_model_1_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_1_reloaded = tf.keras.models.load_model('akt_net_model_1_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add convolution layers to base model\n",
    "model_1_2 = tf.keras.Sequential([\n",
    "    base_model_1,\n",
    "    tf.keras.layers.Dense(512, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'softmax')\n",
    "])\n",
    "\n",
    "model_1_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_2.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1_2 = model_1_1.fit(\n",
    "    ds_training_repeated,\n",
    "    epochs = 10,\n",
    "    steps_per_epoch = 10) # ( None (not with infinite datasets) | int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1_2.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_2.save('akt_net_model_1_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_2_reloaded = tf.keras.models.load_model('akt_net_model_1_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_2 = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top = False,\n",
    "    input_shape = (img_height, img_width, 3),\n",
    "    pooling = 'avg', # ( None | avg | max )\n",
    "    weights = 'imagenet') # ( None | imagenet | path to weights file)\n",
    "\n",
    "for layer in base_model_2.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add convolution layers to base model\n",
    "model_2_1 = tf.keras.Sequential([\n",
    "    base_model_2,\n",
    "    tf.keras.layers.Dense(512, 'relu'),\n",
    "    tf.keras.layers.Dense(256, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'softmax')\n",
    "])\n",
    "\n",
    "model_2_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_1.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_2_1 = model_2_1.fit(\n",
    "    ds_training_repeated,\n",
    "    epochs = 10,\n",
    "    steps_per_epoch = 10) # ( None (not with infinite datasets) | int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2_1.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_1.save('akt_net_model_2_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_1_reloaded = tf.keras.models.load_model('akt_net_model_2_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add convolution layers to base model\n",
    "model_2_2 = tf.keras.Sequential([\n",
    "    base_model_2,\n",
    "    tf.keras.layers.Dense(512, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'softmax')\n",
    "])\n",
    "\n",
    "model_2_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_2.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2_2 = model_2_2.fit(\n",
    "    ds_training_repeated,\n",
    "    epochs = 10,\n",
    "    steps_per_epoch = 10) # ( None (not with infinite datasets) | int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2_2.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_2.save('akt_net_model_2_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_2_reloaded = tf.keras.models.load_model('akt_net_model_2_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1.evaluate(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.predict(ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1(ds_test, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_reloaded.evaluate(ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
